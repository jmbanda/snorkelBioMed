{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Snorkel with biomedical literature and PubAnnotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will try to show how to use snorkel for extraction of related Diseases and Genes from PubMed abstracts using PubAnnotation.\n",
    "\n",
    "The overall flow of this tutorial is the following:\n",
    "1. Use stanford CoreNLP to parse an inital set of 5 documents (which we have labels from DisGeNET).\n",
    "2. Create labeling functions.\n",
    "3. Run labeling functions on the small corpus.\n",
    "4. Compare against gold labels.\n",
    "5. Iterate refining the labeling functions.\n",
    "6. Annotate new corpus and upload to PubAnnotation project.\n",
    "\n",
    "NOTE: A lot of the code is adapted and converted from the Snorkel intro [tutorial](https://github.com/HazyResearch/snorkel/tree/master/tutorials/intro). This is a work in progress and will continue having this warning until the example works end-to-end properly.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a SnorkelSession\n",
    "\n",
    "First, we initialize a SnorkelSession, which manages a connection to a database automatically for us, and will enable us to save intermediate results. If we don't specify any particular database (see commented-out code below), then it will automatically create a SQLite database in the background for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanbanda/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set a global variable related to automatic testing- you can safely ignore this!\n",
    "max_docs = 5 if 'CI' in os.environ else float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the corpus from PubAnnotation\n",
    "\n",
    "Next, we load and pre-process the corpus of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pubannotationutils.getfrompubannotation import getCorpusPubAnnotation\n",
    "getC = getCorpusPubAnnotation('data/test_get.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important step as the last piece of code fetches two things:\n",
    "1. All the text for the specified PMIDs\n",
    "2. All available annotations for the specified project listed as well on test_get.csv. This file is constructed by having an individual PMID,project_name on each line.\n",
    "\n",
    "In this example we get 5 PMID abstracts and all their annotations to use later as gold labels. This the produces 3 different files: \n",
    "1. test_get_text.txt file includes PMID and text.\n",
    "2. test_get_denotations.txt file includes all PubAnnotation denotations.\n",
    "3. test_get_relations.txt file which includes all PubAnnotation relations.\n",
    "\n",
    "We will begin by loading the test_get_text.txt file for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import TSVDocPreprocessor\n",
    "\n",
    "doc_preprocessor = TSVDocPreprocessor('data/test_get_text.txt', max_docs=max_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running a CorpusParser\n",
    "\n",
    "We'll use an NLP preprocessing tool to split our documents into sentences, tokens, and provide annotations--part-of-speech tags, dependency parse structure, lemmatized word forms, etc.--for these sentences.\n",
    "\n",
    "Let's run it single-threaded first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 218 ms, sys: 164 ms, total: 382 ms\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser()\n",
    "%time corpus_parser.apply(doc_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which Snorkel uses) to check how many documents and sentences were parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 5\n",
      "Sentences: 49\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Candidate schema\n",
    "\n",
    "We now define the schema of the relation mention we want to extract (which is also the schema of the candidates). This must be a subclass of Candidate, and we define it using a helper function. Here we'll define a binary Disease-Gene mention which connects two Span objects of text. Note that this function will create the table in the database backend if it does not exist:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "Spouse = candidate_subclass('DiseaseGene', ['disease', 'gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
